import keras
import numpy as np

model = keras.saving.load_model("model6-20.keras")
model.summary()

D = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0909571915631533, 0.1864705988352744, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0.09095719156315263, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0.13333333333333286, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0.19354244752640581, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0.21154318537523054, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0.0257209473385549, 0, 0, 0, 0, 0, 1, 1, 0.2615460252792927, 0.06666666666666754, 0.06666666666666754, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.16575780104032023, 1, 0.9937086792472949, 0.9250323002268592,
     0.9250323002268606, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.8686291501015229, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0.22111145811648902, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
     0.39445405616102835, 0.6666666666666666, 0.2666666666666657, 0.10380497955628054, 0.9333333333333325, 1,
     0.400000000000001, 0, 0, 0, 0.13257158399876245, 0.01803688788671498, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
     1, 0.13290481109172014, 0, 0, 0, 0.8357167202284672, 1, 0.6666666666666666, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0.9416154868018063, 1, 0.2976617387970286, 0, 0, 0, 0.8186636568887105, 1, 0.43085302720802354,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.7599283174841145, 1, 0.6666666666666666, 0, 0, 0,
     0.4885621268327158, 0.980196097281443, 0.16939597096720216, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0.7999999999999996, 1, 0.5333333333333339, 0, 0, 0, 0.8588504820917544, 1, 0.4608804825846402, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.6666666666666666, 1, 0.7353700585195344, 0, 0, 0, 0.7692098292379637, 1,
     0.3932918400931946, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.4741632532352025, 1,
     0.925032300226859, 0, 0, 0, 0, 0.17567059504655913, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0.3333333333333337, 1, 0.8357167202284663, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0.400000000000001, 1, 0.9333333333333325, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0.6402614630419243, 0.2925002033512396, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0]
D = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.8959871176274958, 1, 0.2111456180001683, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0.5850952132544205, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0.58039129021801, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.06666666666666643, 1, 1, 0.26666666666666683, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.025720947338553346, 1, 1, 0.24627380827121303, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.04836251089274035, 1, 1, 0.22111145811648902, 0, 0, 0, 0,
     0.905691491803563, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0.26482149493360674, 0, 0, 0,
     0, 1, 1, 0.07884294123451663, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0.26154602527929405, 0,
     0, 0, 0, 1, 1, 0.07537593858494662, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
     0.26666666666666683, 0, 0, 0, 0, 1, 1, 0.07537593858494662, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     1, 1, 0.20360855540274247, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
     0.331414437981145, 0.3134519145768653, 0.39445405616102835, 0.47976609986781393, 0.5197597792550301, 1, 1,
     0.7855682627481937, 0.925032300226859, 0.9005051665019563, 0.9147452935933522, 1, 0.5577794898144048, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.7580660242992003, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 1, 1, 0.9968051036812438, 0.807430412000111, 0.9250323002268592, 0.7350889359326496, 0.783447493940358,
     1, 1, 0.5215022639328926, 0.39445405616102813, 0.3779299919204757, 0.38754845034029106, 0.3466666666666687, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2666666666666675, 0.3134519145768653, 0, 0, 0, 0, 0.24627380827121192, 1, 1,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.24627380827121192, 1, 1, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.39445405616102813, 1, 0.9845635858848125, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.35075774975293617, 1, 0.8607995006243281, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.24627380827121192, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0]

print(model.layers)
# data = (np.zeros((28,28,1))+0.5)[np.newaxis,...]
data = np.flip(np.array(D).reshape((28, 28, 1)), axis=2)[np.newaxis, ...]
import matplotlib.pyplot as plt

plt.imshow(data[0], cmap='gray')
plt.show()
for layer in model.layers:
    data = layer(data)
    print(layer)
    print(data)
# Testing

input()

# Test the model on the MNIST dataset
# (train, test), info = tfds.load(
#     'mnist',
#     split=['train', 'test'],
#     shuffle_files=True,
#     as_supervised=True,
#     with_info=True,
# )
#
# def normalize_img(image, label):
#     """Normalizes images: `uint8` -> `float32`."""
#     return tf.cast(image, tf.float32) / 255., label
#
# BATCH_SIZE = 128
#
# test = test.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
# test = test.batch(BATCH_SIZE)
# test = test.cache()
# test = test.prefetch(tf.data.AUTOTUNE)
#
# val_loss, val_accuracy = model.evaluate(test)
# print(f"Validation Accuracy: {val_accuracy * 100:.2f}%")


# Get the weights and biases of the model

# for i, layer in enumerate(model.layers):
#     print('\n')
#     print(f"Layer {i}: {layer.name}")
#     weights = model.get_layer(index=i).get_weights()
#     for w, weight in enumerate(weights):
#         print(f"Layer {i}, weight {w}: {weight.shape}")
#         print(f"Layer {i}, weight {w}: {weight}")
#     print('\n')


kernels, biases = model.get_layer(index=0).get_weights()
print(f"Kernels shape: {kernels.shape}")
print(f"Kernels: {kernels}")
L = []
for i in range(kernels.shape[3]):
    L.extend([round(float(w), 8) for w in np.transpose(kernels[:, :, 0, i],(0,1)).flatten()])
    print(f"Kernels {i}: {[float(w) for w in kernels[:, :, 0, i].flatten()]}")
print(L)
print(f"Biases: {biases}")
print(f"Biases: {[float(b) for b in list(biases)]}")

# print('\n\n\n')
#
# weights, biases = model.get_layer(index=7).get_weights()
# print(f"Weights shape: {weights.shape}")
# print(f"Weights: {weights}")
# L = []
# for i in range(weights.shape[1]):
#     L.extend([round(float(w), 8) for w in weights[:, i].flatten()])
#     print(f"Weights {i}: {[round(float(w), 8) for w in weights[:, i].flatten()]}")
# print(L)
# print(f"Biases shape: {biases.shape}")
# print(f"Biases: {[float(b) for b in list(biases)]}")

# Input 10 by 1
# Kernel 10 by 10
# Output 10 by 1
